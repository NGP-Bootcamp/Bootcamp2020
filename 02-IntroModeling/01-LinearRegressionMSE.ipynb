{"nbformat":4,"nbformat_minor":0,"metadata":{"celltoolbar":"Slideshow","colab":{"name":"01-LinearRegressionMSE","provenance":[{"file_id":"https://github.com/NeuromatchAcademy/course-content/blob/NMA2020/tutorials/W1D3_ModelFitting/student/W1D3_Tutorial1.ipynb","timestamp":1600211037716}],"collapsed_sections":[],"toc_visible":true},"kernel":{"display_name":"Python 3","language":"python","name":"python3"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.8"}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"gP7fC-7kAo0R"},"source":["# Model Fitting: Linear regression with MSE\n","\n","**Content creators**: Pierre-Ã‰tienne Fiquet, Anqi Wu, Alex Hyafil with help from Byron Galbraith\n","\n","**Content reviewers**: Lina Teichmann, Saeed Salehi, Patrick Mineault,  Ella Batty, Michael Waskom\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"bVAKnaLnAo0S"},"source":["___\n","#Tutorial Objectives\n","\n","This is Tutorial 1 of a series on fitting models to data. We start with simple linear regression, using least squares optimization (Tutorial 1) and Maximum Likelihood Estimation (Tutorial 2). We will use bootstrapping to build confidence intervals around the inferred linear model parameters (Tutorial 3). We'll finish our exploration of regression models by generalizing to multiple linear regression and polynomial regression (Tutorial 4). We end by learning how to choose between these various models. We discuss the bias-variance trade-off (Tutorial 5) and Cross Validation for model selection (Tutorial 6).\n","\n","In this tutorial, we will learn how to fit simple linear models to data.\n","- Learn how to calculate the mean-squared error (MSE) \n","- Explore how model parameters (slope) influence the MSE\n","- Learn how to find the optimal model parameter using least-squares optimization\n","\n","---\n","\n","**acknowledgements:** \n","- we thank Eero Simoncelli, much of today's tutorials are inspired by exercises asigned in his mathtools class."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"pKGJjIgTAo0T"},"source":["---\n","# Setup"]},{"cell_type":"code","metadata":{"cellView":"both","colab_type":"code","id":"_UjNUsNCAo0U","colab":{}},"source":["import numpy as np\n","import matplotlib.pyplot as plt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","colab_type":"code","id":"LSHQAPrwAo0b","colab":{}},"source":["#@title Figure Settings\n","import ipywidgets as widgets       # interactive display\n","%config InlineBackend.figure_format = 'retina'\n","plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/NMA2020/nma.mplstyle\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","colab_type":"code","id":"pxqZYkXGAo0h","colab":{}},"source":["#@title Helper functions\n","\n","def plot_observed_vs_predicted(x, y, y_hat, theta_hat):\n","  \"\"\" Plot observed vs predicted data\n","\n","  Args:\n","      x (ndarray): observed x values\n","  y (ndarray): observed y values\n","  y_hat (ndarray): predicted y values\n","\n","  \"\"\"\n","  fig, ax = plt.subplots()\n","  ax.scatter(x, y, label='Observed')  # our data scatter plot\n","  ax.plot(x, y_hat, color='r', label='Fit')  # our estimated model\n","  # plot residuals\n","  ymin = np.minimum(y, y_hat)\n","  ymax = np.maximum(y, y_hat)\n","  ax.vlines(x, ymin, ymax, 'g', alpha=0.5, label='Residuals')\n","  ax.set(\n","      title=fr\"$\\hat{{\\theta}}$ = {theta_hat:0.2f}, MSE = {mse(x, y, theta_hat):.2f}\",\n","      xlabel='x',\n","      ylabel='y'\n","  )\n","  ax.legend()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"_F9bjaQ9Ao0p"},"source":["---\n","# Section 1: Mean Squared Error (MSE)"]},{"cell_type":"code","metadata":{"cellView":"form","colab_type":"code","id":"goSvronnAo0q","colab":{}},"source":["#@title Video 1: Linear Regression & Mean Squared Error\n","from IPython.display import YouTubeVideo\n","video = YouTubeVideo(id=\"HumajfjJ37E\", width=854, height=480, fs=1)\n","print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n","video"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"AI94EGdBAo0v"},"source":["**Linear least squares regression** is an old but gold  optimization procedure that we are going to use for data fitting. Least squares (LS) optimization problems are those in which the objective function is a quadratic function of the\n","parameter(s) being optimized.\n","\n","Suppose you have a set of measurements, $y_{n}$ (the \"dependent\" variable) obtained for different input values, $x_{n}$ (the \"independent\" or \"explanatory\" variable). Suppose we believe the measurements are proportional to the input values, but are corrupted by some (random) measurement errors, $\\epsilon_{n}$, that is:\n","\n","$$y_{n}= \\theta x_{n}+\\epsilon_{n}$$\n","\n","for some unknown slope parameter $\\theta.$ The least squares regression problem uses **mean squared error (MSE)** as its objective function, it aims to find the value of the parameter $\\theta$ by minimizing the average of squared errors:\n","\n","\\begin{align}\n","\\min _{\\theta} \\frac{1}{N}\\sum_{n=1}^{N}\\left(y_{n}-\\theta x_{n}\\right)^{2}\n","\\end{align}"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"uvSCSKTpAo0w"},"source":["We will now explore how MSE is used in fitting a linear regression model to data. For illustrative purposes, we will create a simple synthetic dataset where we know the true underlying model. This will allow us to see how our estimation efforts compare in uncovering the real model (though in practice we rarely have this luxury).\n","\n","First we will generate some noisy samples $x$ from [0, 10) along the line $y = 1.2x$ as our dataset we wish to fit a model to."]},{"cell_type":"code","metadata":{"cellView":"form","colab_type":"code","id":"pPl3OkjFAo0w","colab":{}},"source":["# @title\n","\n","# @markdown Execute this cell to generate some simulated data\n","\n","# setting a fixed seed to our random number generator ensures we will always\n","# get the same psuedorandom number sequence\n","np.random.seed(121)\n","\n","# Let's set some parameters\n","theta = 1.2\n","n_samples = 30\n","\n","# Draw x and then calculate y\n","x = 10 * np.random.rand(n_samples)  # sample from a uniform distribution over [0,10)\n","noise = np.random.randn(n_samples)  # sample from a standard normal distribution\n","y = theta * x + noise\n","\n","# Plot the results\n","fig, ax = plt.subplots()\n","ax.scatter(x, y)  # produces a scatter plot\n","ax.set(xlabel='x', ylabel='y');"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ZWjN7KrqAo16"},"source":["Now that we have our suitably noisy dataset, we can start trying to estimate the underlying model that produced it. We use MSE to evaluate how successful a particular slope estimate $\\hat{\\theta}$ is for explaining the data, with the closer to 0 the MSE is, the better our estimate fits the data."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"hrYuU1_tAo18"},"source":["## Exercise 1: Compute MSE\n","\n","In this exercise you will implement a method to compute the mean squared error for a set of inputs $x$, measurements $y$, and slope estimate $\\hat{\\theta}$. We will then compute and print the mean squared error for 3 different choices of theta"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"jfeS9xj0Ao19","colab":{}},"source":["def mse(x, y, theta_hat):\n","  \"\"\"Compute the mean squared error\n","\n","  Args:\n","    x (ndarray): An array of shape (samples,) that contains the input values.\n","    y (ndarray): An array of shape (samples,) that contains the corresponding\n","      measurement values to the inputs.\n","    theta_hat (float): An estimate of the slope parameter\n","\n","  Returns:\n","    float: The mean squared error of the data with the estimated parameter.\n","  \"\"\"\n","  ####################################################\n","  ## TODO for students: compute the mean squared error\n","  # Fill out function and remove\n","  raise NotImplementedError(\"Student exercise: compute the mean squared error\")\n","  ####################################################\n","\n","  # Compute the estimated y\n","  y_hat = ...\n","\n","  # Compute mean squared error\n","  mse = ...\n","\n","  return mse\n","\n","\n","# Uncomment below to test your function\n","theta_hats = [0.75, 1.0, 1.5]\n","# for theta_hat in theta_hats:\n","#   print(f\"theta_hat of {theta_hat} has an MSE of {mse(x, y, theta_hat):.2f}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"cellView":"both","colab_type":"text","outputId":"3ed61eef-3d5e-4118-b0b8-dc2a299c461a","id":"B51Q3FzeAo2F"},"source":["[*Click for solution*](https://github.com/NeuromatchAcademy/course-content/tree/NMA2020/tutorials/W1D3_ModelFitting/solutions/W1D3_Tutorial1_Solution_12a57de0.py)\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"UJtGomfuAo2G"},"source":["The result should be:\n","\n","theta_hat of 0.75 has an MSE of 9.08\\\n","theta_hat of 1.0 has an MSE of 3.0\\\n","theta_hat of 1.5 has an MSE of 4.52\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"R5ru2gpkAo2I"},"source":["We see that $\\hat{\\theta} = 1.0$ is our best estimate from the three we tried. Looking just at the raw numbers, however, isn't always satisfying, so let's visualize what our estimated model looks like over the data. \n","\n"]},{"cell_type":"code","metadata":{"cellView":"form","colab_type":"code","id":"wDWsZzWWAo2J","colab":{