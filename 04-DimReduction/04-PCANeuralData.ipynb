{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 4: Dimensionality Reduction on real datasets\n",
    "## Adapted from Neuromatch Academy: Week 1, Day 5, Tutorial 3 and BIPN162\n",
    "\n",
    "Content creators: Ashley Juavinett, Gal Mishne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial Objectives\n",
    "In the previous tutorial we applied PCA for **Dimensionality reduction** on a simulated dataset. In this notebook we'll run PCA on real neural data.\n",
    "We'll also learn how to use PCA for reconstruction and apply logistic regression to our new data representation.\n",
    "\n",
    "Overview:\n",
    "- Perform PCA on two electrophysiology time series dataset\n",
    "- Plot the results of your dimensionality reduction coloring each point by a known feature \n",
    "- Reconstruct data from PCA\n",
    "- Use dimensionality reduction as a preprocessing step for Logistic regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Video 2: Data Reconstruction\n",
    "from IPython.display import YouTubeVideo\n",
    "video = YouTubeVideo(id=\"ZCUhW26AdBQ\", width=854, height=480, fs=1)\n",
    "print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
    "video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Setup\n",
    "Run these cells to get the tutorial started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import scipy.io\n",
    "from collections import defaultdict\n",
    "import scipy.stats as sc\n",
    "%matplotlib inline\n",
    "import seaborn as sns # This is another plotting package, built really nicely for plotting these types of analyses!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "#@title Data retrieval and loading\n",
    "import os\n",
    "import requests\n",
    "import hashlib\n",
    "\n",
    "url = \"https://osf.io/r9gh8/download\"\n",
    "fname = \"W1D4_steinmetz_data.npz\"\n",
    "expected_md5 = \"d19716354fed0981267456b80db07ea8\"\n",
    "\n",
    "if not os.path.isfile(fname):\n",
    "  try:\n",
    "    r = requests.get(url)\n",
    "  except requests.ConnectionError:\n",
    "    print(\"!!! Failed to download data !!!\")\n",
    "  else:\n",
    "    if r.status_code != requests.codes.ok:\n",
    "      print(\"!!! Failed to download data !!!\")\n",
    "    elif hashlib.md5(r.content).hexdigest() != expected_md5:\n",
    "      print(\"!!! Data download appears corrupted !!!\")\n",
    "    else:\n",
    "      with open(fname, \"wb\") as fid:\n",
    "        fid.write(r.content)\n",
    "\n",
    "def load_steinmetz_data(data_fname=fname):\n",
    "\n",
    "  with np.load(data_fname) as dobj:\n",
    "    data = dict(**dobj)\n",
    "\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "#@title Helper Functions\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def plot_model_selection(C_values, accuracies):\n",
    "  \"\"\"Plot the accuracy curve over log-spaced C values.\"\"\"\n",
    "  ax = plt.figure().subplots()\n",
    "  ax.plot(C_values, accuracies, marker=\"o\")\n",
    "  best_C = C_values[np.argmax(accuracies)]\n",
    "  ax.set(\n",
    "      xticks=C_values,\n",
    "      xlabel=\"$C$\",\n",
    "      ylabel=\"Cross-validated accuracy\",\n",
    "      title=f\"Best C: {best_C:1g} ({np.max(accuracies):.2%})\",\n",
    "  )\n",
    "\n",
    "def get_variance_explained(evals):\n",
    "  \"\"\"\n",
    "  Calculates variance explained from the eigenvalues.\n",
    "\n",
    "  Args:\n",
    "    evals (numpy array of floats) : Vector of eigenvalues\n",
    "\n",
    "  Returns:\n",
    "    (numpy array of floats)       : Vector of variance explained\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  # cumulatively sum the eigenvalues\n",
    "  csum = np.cumsum(evals)\n",
    "  # normalize by the sum of eigenvalues\n",
    "  variance_explained = csum / np.sum(evals)\n",
    "\n",
    "  return variance_explained\n",
    "\n",
    "def plot_data(X):\n",
    "  \"\"\"\n",
    "  Plots bivariate data. Includes a plot of each random variable, and a scatter\n",
    "  scatter plot of their joint activity. The title indicates the sample\n",
    "  correlation calculated from the data.\n",
    "\n",
    "  Args:\n",
    "    X (numpy array of floats) : PCA matrix each column corresponds to a\n",
    "                                different principal component \n",
    "\n",
    "  Returns:\n",
    "    Nothing.\n",
    "  \"\"\"\n",
    "\n",
    "  fig = plt.figure(figsize=[8, 4])\n",
    "  gs = fig.add_gridspec(2, 2)\n",
    "  ax1 = fig.add_subplot(gs[0, 0])\n",
    "  ax1.plot(X[:, 0], color='k')\n",
    "  plt.ylabel('vector 1')\n",
    "  ax2 = fig.add_subplot(gs[1, 0])\n",
    "  ax2.plot(X[:, 1], color='k')\n",
    "  #plt.xlabel('Sample Number (sorted)')\n",
    "  plt.ylabel('vector 2')\n",
    "  ax3 = fig.add_subplot(gs[:, 1])\n",
    "  ax3.plot(X[:, 0], X[:, 1], '.', markerfacecolor=[.5, .5, .5],\n",
    "           markeredgewidth=0)\n",
    "  ax3.axis('equal')\n",
    "  plt.xlabel('PCA 1')\n",
    "  plt.ylabel('PCA 2')\n",
    "  plt.title('Sample corr: {:.1f}'.format(np.corrcoef(X[:, 0], X[:, 1])[0, 1]))\n",
    "  plt.show()\n",
    "\n",
    "    \n",
    "def plot_variance_explained(variance_explained):\n",
    "  \"\"\"\n",
    "  Plots eigenvalues.\n",
    "\n",
    "  Args:\n",
    "    variance_explained (numpy array of floats) : Vector of variance explained\n",
    "                                                 for each PC\n",
    "\n",
    "  Returns:\n",
    "    Nothing.\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  plt.figure()\n",
    "  plt.plot(np.arange(1, len(variance_explained) + 1), variance_explained,\n",
    "           'k')\n",
    "  plt.xlabel('Number of components')\n",
    "  plt.ylabel('Variance explained')\n",
    "  plt.axhline(.9,c='r',ls='--')\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "def change_of_basis(X, W):\n",
    "  \"\"\"\n",
    "  Projects data onto a new basis.\n",
    "\n",
    "  Args:\n",
    "    X (numpy array of floats) : Data matrix each column corresponding to a\n",
    "                                different random variable\n",
    "    W (numpy array of floats) : new orthonormal basis columns correspond to\n",
    "                                basis vectors\n",
    "\n",
    "  Returns:\n",
    "    (numpy array of floats)   : Data matrix expressed in new basis\n",
    "  \"\"\"\n",
    "\n",
    "  Y = np.matmul(X, W)\n",
    "\n",
    "  return Y\n",
    "\n",
    "\n",
    "def get_sample_cov_matrix(X):\n",
    "  \"\"\"\n",
    "  Returns the sample covariance matrix of data X.\n",
    "\n",
    "  Args:\n",
    "    X (numpy array of floats) : Data matrix each column corresponds to a\n",
    "                                different random variable\n",
    "\n",
    "  Returns:\n",
    "    (numpy array of floats)   : Covariance matrix\n",
    "\"\"\"\n",
    "\n",
    "  X = X - np.mean(X, 0)\n",
    "  cov_matrix = 1 / X.shape[0] * np.matmul(X.T, X)\n",
    "  return cov_matrix\n",
    "\n",
    "\n",
    "def sort_evals_descending(evals, evectors):\n",
    "  \"\"\"\n",
    "  Sorts eigenvalues and eigenvectors in decreasing order. Also aligns first two\n",
    "  eigenvectors to be in first two quadrants (if 2D).\n",
    "\n",
    "  Args:\n",
    "    evals (numpy array of floats)    :   Vector of eigenvalues\n",
    "    evectors (numpy array of floats) :   Corresponding matrix of eigenvectors\n",
    "                                         each column corresponds to a different\n",
    "                                         eigenvalue\n",
    "\n",
    "  Returns:\n",
    "    (numpy array of floats)          : Vector of eigenvalues after sorting\n",
    "    (numpy array of floats)          : Matrix of eigenvectors after sorting\n",
    "  \"\"\"\n",
    "\n",
    "  index = np.flip(np.argsort(evals))\n",
    "  evals = evals[index]\n",
    "  evectors = evectors[:, index]\n",
    "  if evals.shape[0] == 2:\n",
    "    if np.arccos(np.matmul(evectors[:, 0],\n",
    "                           1 / np.sqrt(2) * np.array([1, 1]))) > np.pi / 2:\n",
    "      evectors[:, 0] = -evectors[:, 0]\n",
    "    if np.arccos(np.matmul(evectors[:, 1],\n",
    "                           1 / np.sqrt(2)*np.array([-1, 1]))) > np.pi / 2:\n",
    "      evectors[:, 1] = -evectors[:, 1]\n",
    "\n",
    "  return evals, evectors\n",
    "\n",
    "\n",
    "def pca(X):\n",
    "  \"\"\"\n",
    "  Performs PCA on multivariate data. Eigenvalues are sorted in decreasing order\n",
    "\n",
    "  Args:\n",
    "     X (numpy array of floats) :   Data matrix each column corresponds to a\n",
    "                                   different random variable\n",
    "\n",
    "  Returns:\n",
    "    (numpy array of floats)    : Data projected onto the new basis\n",
    "    (numpy array of floats)    : Vector of eigenvalues\n",
    "    (numpy array of floats)    : Corresponding matrix of eigenvectors\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  X = X - np.mean(X, 0)\n",
    "  cov_matrix = get_sample_cov_matrix(X)\n",
    "  evals, evectors = np.linalg.eigh(cov_matrix)\n",
    "  evals, evectors = sort_evals_descending(evals, evectors)\n",
    "  score = change_of_basis(X, evectors)\n",
    "\n",
    "  return score, evectors, evals\n",
    "\n",
    "\n",
    "def plot_eigenvalues(evals, limit=True):\n",
    "  \"\"\"\n",
    "  Plots eigenvalues.\n",
    "\n",
    "  Args:\n",
    "     (numpy array of floats) : Vector of eigenvalues\n",
    "\n",
    "  Returns:\n",
    "    Nothing.\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  plt.figure()\n",
    "  plt.plot(np.arange(1, len(evals) + 1), evals, 'o-k')\n",
    "  plt.xlabel('Component')\n",
    "  plt.ylabel('Eigenvalue')\n",
    "  plt.title('Scree plot')\n",
    "  if limit:\n",
    "    plt.show()\n",
    "    \n",
    "def plot_pca_labels(score,labels):\n",
    "    plt.figure()\n",
    "    plt.scatter(score[:, 0], score[:, 1],c=labels,cmap='jet')\n",
    "    plt.xlabel('PCA 1')\n",
    "    plt.ylabel('PCA 2')\n",
    "    plt.axis('equal')\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "    \n",
    "def plot_data_as_image(data):\n",
    "    fig,ax = plt.subplots(figsize=(15,6))\n",
    "    plt.imshow(data,aspect='auto')\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. Perform PCA on real electrophysiology data\n",
    "\n",
    "\n",
    "We can also use PCA on time series data. Here, we'll work with a sample data set (data_for_exercises.mat) which you can download from [here](https://github.com/marius10p/NeuralDataScienceCSHL2019/blob/master/ByronYuExercises/data_for_exercises.mat). This dataset contains spike counts for 97 neurons recorded using an electrode array over the dorsal premotor cortex of a monkey during a reaching task. Each row of the dataset (728) is a different trial for a different angle of reaching (8 angles total). Trials 1-91 are for reach angle 1, 92-182 for reach angle 2, etc.\n",
    "\n",
    "You can read more about the experiments that created this dataset [here](https://www.jneurosci.org/content/27/40/10742)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numChannels = 97\n",
    "matIn = scipy.io.loadmat('data_for_exercises.mat')\n",
    "spike_data = matIn['Xplan']\n",
    "np.shape(spike_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the raw electrophysiology data here by setting up a plot and looping through each row of `spike_data` to add it to the plot. We add a variable `delt` which increases each time you run through a loop, allowing you to stack all of these traces (instead of plotting them on top of each other)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the raw data here\n",
    "fig,ax = plt.subplots(figsize=(15,10))\n",
    "delt = 0\n",
    "\n",
    "for neuron in np.arange(numChannels):\n",
    "    plt.plot(spike_data[:,neuron]+delt)\n",
    "    delt = delt+10  \n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot the data as an image. Depending on the data it might be easier to idenntify patterns in this display. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the data as an image\n",
    "plot_data_as_image(spike_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In both plots it is hard to determine whether the spike counts ccorrespond to a change in the angle over trials.\n",
    "\n",
    "Here we will use PCA to reduce the dimensioanlity of the trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score, evectors, evals = pca(spike_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we want to plot our trials in the PCA coordinates, also called scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As noted above, *Each row of the dataset (728) is a different trial for a different angle of reaching (8 angles total). Trials 1-91 are for reach angle 1, 92-182 for reach angle 2, etc.* In the previous tutorial of cell features, we colored our final PCA plot by whether the cell was excitatory or inhibitory, and using the kmeans labels. Here, we want to color each point on our PCA plot by the reach angle.\n",
    "\n",
    "To do so, create a vector of \"angles\", which contains a list of angle names (can simply be 0, 1, 2, 3, etc.) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trials = 728\n",
    "angles = np.zeros(n_trials)\n",
    "angle_start = np.arange(0,n_trials,91)\n",
    "angle_end = np.arange(91,n_trials,91)\n",
    "\n",
    "for i in np.arange(len(angle_end)):\n",
    "    angles[angle_start[i]:angle_end[i]] = i\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pca_labels(score,angles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in the previous tutorial our data had a clustered strucutre. Here the PCs reveal more of a continuum corresponding to the angle.  \n",
    "We can see this information on angles also appears (to lesser extent) in \"deeper\" PCs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################\n",
    "## TO DO for students: plot the total variance explained\n",
    "# Comment once you've filled in the function\n",
    "raise NotImplementedError(\"Student excercise: plotting PCs 3-4!\")\n",
    "\n",
    "# Plot prinicipal components 3 and 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Reconstruct data with different numbers of PCs\n",
    "\n",
    "Now we have seen that the top 40 or so principal components of the data can explain most of the variance. We can use this fact to perform *dimensionality reduction*, i.e., by storing the data using only 40 components rather than the samples of all the neurons. Remarkably, we will be able to reconstruct much of the structure of the data using only the top 40 components. To see this, recall that to perform PCA we projected the data $\\bf X$ onto the eigenvectors of the covariance matrix:\n",
    "\\begin{equation}\n",
    "\\bf S = X W\n",
    "\\end{equation}\n",
    "Since $\\bf W$ is an orthogonal matrix, ${\\bf W}^{-1} = {\\bf W}^T$. So by multiplying by ${\\bf W}^T$ on each side we can rewrite this equation as  \n",
    "\\begin{equation}\n",
    "{\\bf X = S W}^T.\n",
    "\\end{equation}\n",
    "This now gives us a way to reconstruct the data matrix from the scores and loadings. To reconstruct the data from a low-dimensional approximation, we just have to truncate these matrices.  Let's call ${\\bf S}_{1:K}$ and ${\\bf W}_{1:K}$ as keeping only the first $K$ columns of this matrix. Then our reconstruction is:\n",
    "\\begin{equation}\n",
    "{\\bf \\hat X = S}_{1:K} ({\\bf W}_{1:K})^T.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise:\n",
    "Fill in the function below to reconstruct the data using different numbers of principal components.\n",
    "\n",
    "Steps:\n",
    "\n",
    "- Fill in the following function to reconstruct the data based on the weights and scores. Don't forget to add the mean!\n",
    "- Make sure your function works by reconstructing the data with all K=784 components. The two images should look identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_data(score, evectors, X_mean, K):\n",
    "  \"\"\"\n",
    "  Reconstruct the data based on the top K components.\n",
    "  Args:\n",
    "    score (numpy array of floats)    : Score matrix\n",
    "    evectors (numpy array of floats) : Matrix of eigenvectors\n",
    "    X_mean (numpy array of floats)   : Vector corresponding to data mean\n",
    "    K (scalar)                       : Number of components to include\n",
    "  Returns:\n",
    "    (numpy array of floats)          : Matrix of reconstructed data\n",
    "  \"\"\"\n",
    "\n",
    "  #################################################\n",
    "  ## TO DO for students: Reconstruct the original data in X_reconstructed\n",
    "  # Comment once you've filled in the function\n",
    "  raise NotImplementedError(\"Student excercise: reconstructing data function!\")\n",
    "  #################################################\n",
    "\n",
    "  # Reconstruct the data from the score and eigenvectors\n",
    "  # Don't forget to add the mean!!\n",
    "\n",
    "  X_reconstructed =  \n",
    "\n",
    "  return X_reconstructed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how well the data is reconstructed for different number of stored compomnents. Below calculate the reconstruction error for ncreasing number of PCs\n",
    "\\begin{equation}\n",
    "\\text{MSE}= \\frac{1}{\\text{# trials}}\\sum_\\text{trial} \\Vert {\\mathbf{X}[:,\\text{trial}] -  \\hat{\\mathbf{X}}[:,\\text{trial}]} \\Vert_2^2 = \\frac{1}{\\text{# trials}} \\Vert \\bf{X} -  \\hat{\\bf{X}} \\Vert_F^2\n",
    "\\end{equation}\n",
    "The squared reconstruction error is the average squared reconstruction error of all trials. This can also be calculated in matrix form using the Frobenius norm:   \n",
    "\\begin{equation}\n",
    "\\Vert \\textbf{X} \\Vert_F^2 = \\sum_{i,j} |  \\textbf{X}[i,j]|^2\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_errors = []\n",
    "C_values = np.array([1,2, 5, 10, 20, 30, 50, 60, 70, 80, 90, 100, 120])\n",
    "for C in C_values:\n",
    "    # Initialize and fit the model\n",
    "        \n",
    "    #################################################\n",
    "    ## TO DO for students: Reconstruct the spike data \n",
    "    # Comment once you've filled in the function\n",
    "    raise NotImplementedError(\"Student excercise: reconstructing data!\")\n",
    "    #################################################\n",
    "    \n",
    "    spike_data_mean = \n",
    "\n",
    "    reconstructed = \n",
    "    recon_error = np.linalg.norm(reconstructed-spike_data,'fro')**2 / np.shape(spike_data)[1]\n",
    "    rec_errors.append(recon_error.mean())\n",
    "\n",
    "# Uncomment to plot the reconstruction errors \n",
    "# plt.plot(C_values, rec_errors)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does the reconstruction error compare to the variance explained? What trend do you notice in comapring the two?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################\n",
    "## TO DO for students: plot the total variance explained\n",
    "# Comment once you've filled in the function\n",
    "raise NotImplementedError(\"Student excercise: reconstructing data function!\")\n",
    "\n",
    "var_explained = \n",
    "\n",
    "# Uncomment to plot variance explained\n",
    "# plot_variance_explained(var_explained)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see what the reconstructed data looks like and how it differs from the original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  #################################################\n",
    "  ## TO DO for students: Reconstruct the spike data for K number of principal components. try different values of K\n",
    "  # Comment once you've filled in the function\n",
    "  # raise NotImplementedError(\"Student excercise: reconstructing data!\")\n",
    "  #################################################\n",
    "reconstructed = \n",
    "\n",
    "# Uncomment to plot the reconstructed data and  errors \n",
    "# plot_data_as_image(reconstructed) # note negative values\n",
    "# plot_data_as_image((spike_data-reconstructed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3. Perform PCA on Neuropixel data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider a subset of data from a study of Steinmetz et al. (2019). In this study, Neuropixels probes were implanted in the brains of mice. Electrical potentials were measured by hundreds of electrodes along the length of each probe. Each electrode's measurements captured local variations in the electric field due to nearby spiking neurons. A spike sorting algorithm was used to infer spike times and cluster spikes according to common origin: a single cluster of sorted spikes is causally attributed to a single neuron.\n",
    "\n",
    "In particular, a single recording session of spike times and neuron assignments was loaded and assigned to spike_times in the preceding setup.\n",
    "\n",
    "This dataset includes recordings of neurons as mice perform a decision task.\n",
    "Mice had the task of turning a wheel to indicate whether they perceived a Gabor stimulus to the left, to the right, or not at all. Neuropixel probes measured spikes across the cortex.  We will only consider trials where the mouse chose \"Left\" or \"Right\" and ignore NoGo trials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the hidden `Data retrieval and loading` cell, there is a function that loads the data:\n",
    "\n",
    "- `spikes`: an array of normalized spike rates with shape `(n_trials, n_neurons)`\n",
    "- `choices`: a vector of 0s and 1s, indicating the animal's behavioral response, with length `n_trials`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "data = load_steinmetz_data()\n",
    "for key, val in data.items():\n",
    "  print(key, val.shape)\n",
    "\n",
    "y = data[\"choices\"]\n",
    "X = data[\"spikes\"] # trials x neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot the spiking rate data here\n",
    "plot_data_as_image(X) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Perform PCA on trials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first reduce dimensionality of the trials. \n",
    "Apply PCA to the *spikes* using the helper functions, and then the first two PCs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  #################################################\n",
    "  ## TO DO for students: calculate PCA\n",
    "  # Comment once you've filled in the function\n",
    "  raise NotImplementedError(\"Student excercise: apply PCA to spike data!\")\n",
    "score, evectors, evals = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use helper function to plot PCs \n",
    "plot_data(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to see if the choice of the trial is evident in the PCs. We can also see from PC 2 that it has a decreasing trend over trial index.\n",
    "Color first two PCs by trial index and choice using *plot_pca_labels* helper function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# color PC using choice in the trials\n",
    "plot_pca_labels(score,labels = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  #################################################\n",
    "  ## TO DO for students: color points by trial index\n",
    "  # Comment once you've filled in the function\n",
    "  raise NotImplementedError(\"Student excercise: color PCA by trial index!\")\n",
    "\n",
    "# color PC using trial index\n",
    "trial_index = \n",
    "\n",
    "# uncomment to plot PCA\n",
    "# plot_pca_labels(score,labels = trial_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot next two PCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pca_labels(score[:,2:],y)\n",
    "plot_pca_labels(score[:,2:],trial_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that both the trial index and the trial choice are apparent in the first few PCs. Can we use this low-dim representation to predict the choice?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Logistic regression and model validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We learned about logistic regression in the Machine learning tutorial yesterday. A common use of dimensionality reduction is to obtain a new representation of the data in an unsupervised manner, and tehn apply a machine learning method (such as classification) to this new representation. \n",
    "\n",
    "Here we will apply logistic regression to the new PCA coordinates and see if there is an \"optimal\" dimension of the data that reflects the choice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_selection(X, y, score, C_values):\n",
    "  \"\"\"Compute CV accuracy for each C value.\n",
    "  Args:\n",
    "    X (2D array): Data matrix\n",
    "    y (1D array): Label vector\n",
    "    C_values (1D array): Array of hyperparameter values.\n",
    "  Returns:\n",
    "    accuracies (1D array): CV accuracy with each value of C.\n",
    "  \"\"\"\n",
    "  accuracies = []\n",
    "  for C in C_values:\n",
    "    \n",
    "  #################################################\n",
    "  ## TO DO for students: limit the number of prinicipal components used in the model \n",
    "  # Comment once you've filled in the function\n",
    "  raise NotImplementedError(\"logistic regression on PCA\")\n",
    "    proj_data = score[:,:C]  ####\n",
    "\n",
    "    model = LogisticRegression(penalty=\"none\", max_iter=1000)\n",
    "\n",
    "    # Get the accuracy for each test split\n",
    "    accs = cross_val_score(model, proj_data, y, cv=8)\n",
    "\n",
    "    # Store the average test accuracy for this value of C\n",
    "    accuracies.append(accs.mean())\n",
    "\n",
    "  return accuracies\n",
    "\n",
    "# set C values\n",
    "C_values = np.array([1, 2, 3, 5, 7, 10, 20, 30, 50, 75, 100, 125, 150])\n",
    "\n",
    "accuracies = model_selection(X, y, score, C_values)\n",
    "# uncomment to plot accuracy \n",
    "# plot_model_selection(C_values, accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at variance explained plot. How does it correspond to the model selection? Compare the number of PCs necessary to recover 90% of the total variance to the number of PCs that result in best classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variance_explained = get_variance_explained(evals)\n",
    "plot_variance_explained(variance_explained[:150])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Perform PCA on the neurons\n",
    "Run PCA on other dimension of the data (reducing dimension of the neurons) and plot variance explained. Is the same number of dimensions used to explain 90% of the variance in the data? Why is there a difference? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################\n",
    "  ## TO DO for students: apply PCA to neurons \n",
    "  # Comment once you've filled in the function\n",
    "  raise NotImplementedError(\"PCA on neurons\")\n",
    "    \n",
    "Xs =         # neurons x trials\n",
    "\n",
    "score, evectors, evals = pca(Xs)\n",
    "\n",
    "# uncomment to plot variance explained\n",
    "# variance_explained = get_variance_explained(evals)\n",
    "# plot_variance_explained(variance_explained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(score[:,:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot PCs colored by mean value of spike rates\n",
    "plot_pca_labels(score,np.mean(Xs,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "collapsed_sections": [],
   "name": "04-PCANeuralData",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
