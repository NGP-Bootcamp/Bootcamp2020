{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 3: Dimensionality Reduction and clustering\n",
    "## Adapted from Neuromatch Academy: Week 1, Day 5, Tutorial 3 and BIPN162\n",
    "\n",
    "The examples derived here are largely from Pascal Wallisch's *Neural Data Science* (Chapter 8), and Jake VanderPlaas's *Neural Data Science Handbook*. \n",
    "Content creators: Ashley Juavinett, Gal Mishne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial Objectives\n",
    "**Dimensionality reduction** is an incredibly useful tool in data science as well as neuroscience. Here, we'll explore one main method of dimensionality reduction: **Principal Components Analysis**. We'll first perform PCA step-by-step on a simulated dataset, and then learn the very simple commands to do this in Python. Next, we'll use *k*-means clustering to find clusters in the two-dimensional PCA data. \n",
    "\n",
    "In this notebook we'll learn to apply PCA for dimensionality reduction.\n",
    "\n",
    "Overview:\n",
    "\n",
    "- Perform PCA on a simulated neuroscience datasets\n",
    "- Calculate the variance explained\n",
    "- Use *k*-means clustering to find clusters in your data\n",
    "- Plot the results of your dimensionality reduction and clustering, coloring each point by either a known feature or by a cluster assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "Run these cells to get the tutorial started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "%matplotlib inline\n",
    "import seaborn as sns # This is another plotting package, built really nicely for plotting these types of analyses!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "#@title Helper Functions\n",
    "\n",
    "def plot_covariance(covar):\n",
    "    plt.imshow(covar)\n",
    "    plt.colorbar()\n",
    "    plt.title('Covariance Matrix (\\u03A3)')\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "def plot_variance_explained(variance_explained):\n",
    "  \"\"\"\n",
    "  Plots eigenvalues.\n",
    "\n",
    "  Args:\n",
    "    variance_explained (numpy array of floats) : Vector of variance explained\n",
    "                                                 for each PC\n",
    "\n",
    "  Returns:\n",
    "    Nothing.\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  plt.figure()\n",
    "  plt.plot(np.arange(1, len(variance_explained) + 1), variance_explained,\n",
    "           'k')\n",
    "  plt.xlabel('Number of components')\n",
    "  plt.ylabel('Variance explained')\n",
    "  plt.axhline(.9,c='r',ls='--')\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "def change_of_basis(X, W):\n",
    "  \"\"\"\n",
    "  Projects data onto a new basis.\n",
    "\n",
    "  Args:\n",
    "    X (numpy array of floats) : Data matrix each column corresponding to a\n",
    "                                different random variable\n",
    "    W (numpy array of floats) : new orthonormal basis columns correspond to\n",
    "                                basis vectors\n",
    "\n",
    "  Returns:\n",
    "    (numpy array of floats)   : Data matrix expressed in new basis\n",
    "  \"\"\"\n",
    "\n",
    "  Y = np.matmul(X, W)\n",
    "\n",
    "  return Y\n",
    "\n",
    "\n",
    "def get_sample_cov_matrix(X):\n",
    "  \"\"\"\n",
    "  Returns the sample covariance matrix of data X.\n",
    "\n",
    "  Args:\n",
    "    X (numpy array of floats) : Data matrix each column corresponds to a\n",
    "                                different random variable\n",
    "\n",
    "  Returns:\n",
    "    (numpy array of floats)   : Covariance matrix\n",
    "\"\"\"\n",
    "\n",
    "  X = X - np.mean(X, 0)\n",
    "  cov_matrix = 1 / X.shape[0] * np.matmul(X.T, X)\n",
    "  return cov_matrix\n",
    "\n",
    "\n",
    "def sort_evals_descending(evals, evectors):\n",
    "  \"\"\"\n",
    "  Sorts eigenvalues and eigenvectors in decreasing order. Also aligns first two\n",
    "  eigenvectors to be in first two quadrants (if 2D).\n",
    "\n",
    "  Args:\n",
    "    evals (numpy array of floats)    :   Vector of eigenvalues\n",
    "    evectors (numpy array of floats) :   Corresponding matrix of eigenvectors\n",
    "                                         each column corresponds to a different\n",
    "                                         eigenvalue\n",
    "\n",
    "  Returns:\n",
    "    (numpy array of floats)          : Vector of eigenvalues after sorting\n",
    "    (numpy array of floats)          : Matrix of eigenvectors after sorting\n",
    "  \"\"\"\n",
    "\n",
    "  index = np.flip(np.argsort(evals))\n",
    "  evals = evals[index]\n",
    "  evectors = evectors[:, index]\n",
    "  if evals.shape[0] == 2:\n",
    "    if np.arccos(np.matmul(evectors[:, 0],\n",
    "                           1 / np.sqrt(2) * np.array([1, 1]))) > np.pi / 2:\n",
    "      evectors[:, 0] = -evectors[:, 0]\n",
    "    if np.arccos(np.matmul(evectors[:, 1],\n",
    "                           1 / np.sqrt(2)*np.array([-1, 1]))) > np.pi / 2:\n",
    "      evectors[:, 1] = -evectors[:, 1]\n",
    "\n",
    "  return evals, evectors\n",
    "\n",
    "\n",
    "def pca(X):\n",
    "  \"\"\"\n",
    "  Performs PCA on multivariate data. Eigenvalues are sorted in decreasing order\n",
    "\n",
    "  Args:\n",
    "     X (numpy array of floats) :   Data matrix each column corresponds to a\n",
    "                                   different random variable\n",
    "\n",
    "  Returns:\n",
    "    (numpy array of floats)    : Data projected onto the new basis\n",
    "    (numpy array of floats)    : Vector of eigenvalues\n",
    "    (numpy array of floats)    : Corresponding matrix of eigenvectors\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  X = X - np.mean(X, 0)\n",
    "  cov_matrix = get_sample_cov_matrix(X)\n",
    "  evals, evectors = np.linalg.eigh(cov_matrix)\n",
    "  evals, evectors = sort_evals_descending(evals, evectors)\n",
    "  score = change_of_basis(X, evectors)\n",
    "\n",
    "  return score, evectors, evals\n",
    "\n",
    "\n",
    "def plot_eigenvalues(evals, limit=True):\n",
    "  \"\"\"\n",
    "  Plots eigenvalues.\n",
    "\n",
    "  Args:\n",
    "     (numpy array of floats) : Vector of eigenvalues\n",
    "\n",
    "  Returns:\n",
    "    Nothing.\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  plt.figure()\n",
    "  plt.plot(np.arange(1, len(evals) + 1), evals, 'o-k')\n",
    "  plt.xlabel('Component')\n",
    "  plt.ylabel('Eigenvalue')\n",
    "  plt.title('Scree plot')\n",
    "  if limit:\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def plot_eigenvectors(eigenvectors, feature_names):\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.imshow(eigenvectors,cmap='viridis',)\n",
    "    plt.yticks([0,1,2],['PC1','PC2','PC3'],fontsize=10)\n",
    "    plt.colorbar(orientation='horizontal')\n",
    "    plt.tight_layout()\n",
    "    plt.xticks(range(len(feature_names)),feature_names,rotation=65)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_pca_labels(score,labels):\n",
    "    plt.figure()\n",
    "    plt.scatter(score[:, 0], score[:, 1],c=labels,cmap='jet')\n",
    "    plt.xlabel('PCA 1')\n",
    "    plt.ylabel('PCA 2')\n",
    "    plt.axis('equal')\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "    \n",
    "def plot_pca_labels_with_centroids(score,labels,centers):\n",
    "    plt.figure()\n",
    "    plt.scatter(score[:, 0], score[:, 1],c=labels,cmap='jet')\n",
    "    plt.xlabel('PCA 1')\n",
    "    plt.ylabel('PCA 2')\n",
    "    plt.axis('equal')\n",
    "    plt.colorbar()\n",
    "    plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Load a dataset of 100 neurons with eight different features.\n",
    "\n",
    "Here, we will load a dataset with eight different features: response latency, somatic volume, cortical depth, maximum firing rate, spontaneous firing rate, spike width, axon length, and dendritic arborization area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"simulated_cells2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the next cell to check what is the size of the dataset, and see the first few datapoints in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows,ncolumns = dataset.shape\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at these eight different factors, and how they compare. We will use *Seaborn*, a Python data visualization library based on matplotlib. It provides a high-level interface for drawing informative statistical graphics. We will use *pairplot* to plot pairwise relationships in a dataset. The diagonal plots show the univariate distribution of the data for the variable in that column. The off-diagonal plots are pairwise scatter plots of the features in the data. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(dataset, height=1.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, there's a lot going on here. We'll want to use dimensionality reduction to try to understand if we can reduce the understanding of this dataset into a couple of different factors, and highlight which features succintly describe the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, we'll drop \"transmission\" from our dataframe -- we don't want to run PCA on it. Then, we'll normalize each column of data. Normalization isn't strictly *necessary* but it is a really useful for a dataset like this one where different variables have wildly different values (e.g., 0.5 to 300)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To normnalize the data, subtract the mean from each column and divide by the standard deviation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = dataset.drop('transmission',axis=1)\n",
    "\n",
    "  #################################################\n",
    "  ## TO DO for students: normalize the columns of the data\n",
    "  # Comment once you've filled in the function\n",
    "  raise NotImplementedError(\"Student excercise: normalize data!\")\n",
    "  #################################################\n",
    "\n",
    "x_data = \n",
    "\n",
    "# Uncomment to see the first few rows of the dataset\n",
    "# x_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Perform PCA\n",
    "\n",
    "### 1. Compute the covariance matrix.\n",
    "\n",
    "Here, we'll calculate the **covariance** between the columns of our dataset using our *get_sample cov* function. Since we have normalized our columns, this is a correlation matrix (values are limited between -1 and 1). A positive correaltion means these columns vary together, whereas a negative correaltion means these columns vary in opposite directions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert pandas dataframe to numpy array so we can use functions we developed in previous tutorials \n",
    "X = x_data.to_numpy()\n",
    "\n",
    "  #################################################\n",
    "  ## TO DO for students: calculate the sample covariance of the dataset\n",
    "  # Comment once you've filled in the function\n",
    "  raise NotImplementedError(\"Student excercise: calculate covariance!\")\n",
    "  #################################################\n",
    "    \n",
    "cov_mat = \n",
    "\n",
    "# Uncomment to plot the covariance matrix\n",
    "# plot_covariance(cov_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Scree plot of PCA\n",
    "\n",
    "PCA will extract as many factors as there are variables, but we'd like to know how many of these factors are useful in explaining our data. \n",
    "\n",
    "There are a few different ways to do this (e.g., a Kaiser criterion), but here we'll use a \"[Scree plot](https://en.wikipedia.org/wiki/Scree_plot)\", and look for an \"elbow\" (very scientific, I know). Factors to the left of the elbow are considered meaningful, whereas factors to the right are considered noise.\n",
    "\n",
    "**Steps:**\n",
    "- Perform PCA on the dataset and examine the scree plot. \n",
    "- When do the eigenvalues appear (by eye) to reach zero? (**Hint:** use `plt.xlim` to zoom into a section of the plot)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(pca)\n",
    "help(plot_eigenvalues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################\n",
    "## TO DO for students: perform PCA and plot the eigenvalues\n",
    "# Comment once you've filled in the function\n",
    "raise NotImplementedError(\"Student excercise: apply PCA to data!\")\n",
    "#################################################\n",
    "\n",
    "# perform PCA\n",
    "score, evectors, evals = \n",
    "# Uncomment to plot the eigenvalues\n",
    "# plot_eigenvalues(evals, limit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: Calculate the variance explained\n",
    "\n",
    "The scree plot suggests that most of the eigenvalues are near zero, with fewer than 100 having large values. Another common way to determine the intrinsic dimensionality is by considering the variance explained. This can be examined with a cumulative plot of the fraction of the total variance explained by the top $K$ components, i.e.,\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{var explained} = \\frac{\\sum_{i=1}^K \\lambda_i}{\\sum_{i=1}^N \\lambda_i}\n",
    "\\end{equation}\n",
    "\n",
    "The intrinsic dimensionality is often quantified by the $K$ necessary to explain a large proportion of the total variance of the data (often a defined threshold, e.g., 90%)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Plot the explained variance\n",
    "\n",
    "In this exercise you will plot the explained variance. \n",
    "\n",
    "**Steps:**\n",
    "- Fill in the function below to calculate the fraction variance explained as a function of the number of principal componenets. **Hint:** use `np.cumsum`.\n",
    "- Plot the variance explained using `plot_variance_explained`.\n",
    "\n",
    "**Questions:**\n",
    "- How many principal components are required to explain 90% of the variance?\n",
    "- How does the intrinsic dimensionality of this dataset compare to its extrinsic dimensionality?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(plot_variance_explained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_variance_explained(evals):\n",
    "  \"\"\"\n",
    "  Calculates variance explained from the eigenvalues.\n",
    "\n",
    "  Args:\n",
    "    evals (numpy array of floats) : Vector of eigenvalues\n",
    "\n",
    "  Returns:\n",
    "    (numpy array of floats)       : Vector of variance explained\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  #################################################\n",
    "  ## TO DO for students: calculate the explained variance using the equation\n",
    "  ## from Section 2.\n",
    "  # Comment once you've filled in the function\n",
    "  raise NotImplementedError(\"Student excercise: calculate explaine variance!\")\n",
    "  #################################################\n",
    "\n",
    "  # cumulatively sum the eigenvalues\n",
    "  csum = \n",
    "  # normalize by the sum of eigenvalues\n",
    "  variance_explained = \n",
    "\n",
    "  return variance_explained\n",
    "\n",
    "\n",
    "#################################################\n",
    "## TO DO for students: call the function and plot the variance explained\n",
    "#################################################\n",
    "\n",
    "# calculate the variance explained\n",
    "variance_explained = get_variance_explained(evals)\n",
    "\n",
    "# Uncomment to plot the variance explained and 90% of the variance explained\n",
    "# plot_variance_explained(variance_explained)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we would need 5-6 principal components to account for more than 90% of the variance in the dataset (which would be a lot)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another thing we can do is look at the explained variance for each PCA.\n",
    "## TO DO for students: calculate variance of each prinicipal component (is there mofre than one way you can do this?)\n",
    "#################################################\n",
    "\n",
    "ex_variance_ratio = \n",
    "\n",
    "# Uncomment to print variance explained of each component \n",
    "# print(ex_variance_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Interpreting the meaning of factors.\n",
    "This part is really tricky for neural data -- what counts as a *meaningful* factor for brain activity? This part is more or less easy depending on your data and how much you already know about it. \n",
    "\n",
    "For spike sorting, these factors are typically something obvious like spike amplitude or spike width, but for behavioral measures or population dynamics, the meaning of factors could be less obvious. Below, we can see how much each feature contributes to the first PCs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_eigenvectors(evectors[:,:3].T, x_data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Determining the factor values of the original variables\n",
    "\n",
    "Here, we'll plot the first two dimensions of our PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excitatory = dataset.transmission=='excitatory'\n",
    "plot_pca_labels(score,excitatory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember the PCA didn't have any information on whether our cells were excitatory or inhibitory, but clearly it's picking up on features that also divide those cell types. And, it looks like excitatory and inhibitory cells actually might fall into three classes. Let's see what happens if we use *k*-means clustering on our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans #Import the KMeans model\n",
    "\n",
    "#################################################\n",
    "## TO DO for students: # Set up a kmeans model with 3 clusters\n",
    "# Comment once you've filled in the function\n",
    "  raise NotImplementedError(\"Student excercise: apply kmeans!\")\n",
    "#################################################\n",
    "n_clust = \n",
    "kmeans = KMeans(n_clusters=n_clust) \n",
    "\n",
    "kmeans.fit(score[:,:2]) # Fit our two dimensional data\n",
    "labels_kmeans = kmeans.predict(score[:,:2]) \n",
    "\n",
    "# uncomment to plot the first 2PCs with points colored by kmeans labels and the corresponding centroids\n",
    "# plot_pca_labels_with_centroids(score,labels_kmeans,kmeans.cluster_centers_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we tell *k*-means to use three clusters, it divides up the data into three clusters. What happens if we tell it there are two clusters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans #Import the KMeans model\n",
    "\n",
    "# Set up a kmeans model with 2 clusters\n",
    "n_clust = \n",
    "kmeans = KMeans(n_clusters=n_clust) \n",
    "\n",
    "kmeans.fit(score[:,:2]) # Fit our two dimensional data\n",
    "labels_kmeans_2 = kmeans.predict(score[:,:2]) \n",
    "centers = kmeans.cluster_centers_\n",
    "\n",
    "# uncomment to plot the first 2PCs with points colored by kmeans labels and the corresponding centroids\n",
    "# plot_pca_labels_with_centroids(score,y_kmeans_2,centers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Biplot\n",
    "\n",
    "A loading plot shows how strongly each feature influences a principal component.\n",
    "A biplot overlays this on top of the plotted prinicpal components.\n",
    "\n",
    "- Plot biplot to figure out contributions of each feature to the prinicipal components.\n",
    "- Based on the biplot, plot first 2PCs colored by meaningful features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following function plots the first two prinicipal components (scores) and the loading plot of the features\n",
    "def biplot(score,coeff,labels=None):\n",
    "    xs = score[:,0]\n",
    "    ys = score[:,1]\n",
    "    n = coeff.shape[0]\n",
    "    scalex = 1.0/(xs.max() - xs.min())\n",
    "    scaley = 1.0/(ys.max() - ys.min())\n",
    "    plt.scatter(xs * scalex,ys * scaley,s=5)\n",
    "    for i in range(n):\n",
    "        plt.arrow(0, 0, coeff[i,0], coeff[i,1],color = 'r',alpha = 0.5)\n",
    "        if labels is None:\n",
    "            plt.text(coeff[i,0]* 1.15, coeff[i,1] * 1.15, \"Var\"+str(i+1), color = 'green', ha = 'center', va = 'center')\n",
    "        else:\n",
    "            plt.text(coeff[i,0]* 1.15, coeff[i,1] * 1.15, labels[i], color = 'g', ha = 'center', va = 'center')\n",
    " \n",
    "    plt.xlabel(\"PC{}\".format(1))\n",
    "    plt.ylabel(\"PC{}\".format(2))\n",
    "    plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biplot(score,evectors,list(x_data.columns))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the biplot, choose three features which if you color the cells by, you get a meaningful separation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in 3 feature names below and then run the following three cells\n",
    "feat1 = \n",
    "feat2 = \n",
    "feat3 = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pca_labels(score,feat1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pca_labels(score,feat2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pca_labels(score,feat3)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "collapsed_sections": [],
   "name": "03-PCAandClustering",
   "provenance": [],
   "toc_visible": true
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
